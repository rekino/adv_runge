\documentclass[12pt,onecolumn,a4paper]{article}
\usepackage{epsfig,graphicx,amsthm,amsmath,amsfonts}
\usepackage{color,xcolor}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepackage{tikz}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{xepersian}
\settextfont{XB Zar}
\setlatintextfont{Times New Roman}

\newtheorem{theorem}{قضیه}[section]
\newtheorem{corollary}{نتیجه‌}[theorem]

\pgfplotstableread[col sep = comma]{gibbs.csv}\gibbs
\pgfplotstableread{
-1 0
-1 -1
0 -1
0 1
1 1
1 0
}\gibbsref
% \pgfplotstableread[col sep = comma]{adv_phenom_oned.txt}\signoned
% \pgfplotstableread[col sep = comma]{adv_phenom_mono.txt}\signmono
% \pgfplotstableread[col sep = comma]{adv_phenom_cheby.txt}\signcheby
% \pgfplotstableread[col sep = comma]{adv_phenom_legen.txt}\signlegen
% \pgfplotstableread[col sep = comma]{adv_phenom_samples.txt}\signmemory

\begin{document}
\title{توجیه وجود مثال‌های خصمانه \\ و انتقال‌پذیری آن‌ها} 
\author{رامین براتی}
\date{\today}
\maketitle

\begin{abstract}
یادگیری عمیق در مرکز پیشرفت‌های اخیر در یادگیری ماشین و هوش مصنوعی قرار دارد. با این که این مدل‌ها توانایی فراانسانی در انجام بعضی کاربردها از خود نشان می‌دهند، مطالعات نشان داده است که این مدل‌ها به تغییرات کوچک و غیر قابل تشخیص در ورودی حساس هستند. این اختلالات را اختلالات خصمانه نام‌گذاری کرده‌اند. پدیده‌ی اختلالات خصمانه تهدید قابل توجهی برای استفاده از مدل‌های یادگیری ماشین در کاربردهای نیازمند ایمنی است و به همین دلیل، تحقیقات بسیاری در این مسیر انجام شده است. در این مقاله، دیدگاه جدیدی در مورد دلیل وجود این پدیده ارائه می‌دهیم. با تکیه بر این دیدگاه، روشی برای ایمن‌سازی شبکه‌های عصبی به حملات خصمانه ارائه می‌دهیم. نتایج اولیه حاکی از موثر بودن روش پیشنهادی در کاهش قابل توجه حساسیت شبکه‌های عصبی به مثال‌های خصمانه است. در عین حال، پیاده‌سازی روش پیشنهادی وابستگی‌ای به شبکه‌های عصبی ندارد و به راحتی قابل انتقال به دیگر مدل‌های یادگیری ماشین نیز هست.
\end{abstract}

\section{مقدمه} 
با پیشرفت‌هایی که اخیرا در الگوریتم‌ها و قدرت محاسباتی به وجود آمده، یادگیری ماشین به یکی از پرکاربردترین ابزارها در صنایع مختلف تبدیل شده است. بدون شک، ظهور و معرفی آموزش عمیق به همراه وجود مجموعه‌های بزرگ آموزشی را می‌توان مسئول بخش قابل توجهی از این پیشرفت‌ها دانست. موفقیت‌ کاربرد آموزش عمیق در مسائل طبقه‌بندی تصاویر، شناسایی اشیا، پردازش زبان طبیعی و دیگر کاربردهای متنوع جایگاه شبکه‌های عصبی چندلایه در صنایع مختلف را نیز تثبیت کرده است.

با توجه به این پیشرفت‌ها، طبیعی است که بخواهیم از این مدل‌ها در کاربردهایی مانند ماشین‌های خودران و دیگر کاربردهایی که امنیت در آنها اهمیت دارد استفاده کنیم. با این حال، در
\cite{szegedy2013intriguing} 
نشان داده شد که مدلهای عمیق به پدیده‌ی مثال‌های خصمانه حساس هستند. مثال‌های خصمانه نقاطی در همسایگی مثال‌های طبیعی هستند که طبقه‌بند در آن نقاط خروجی اشتباه می‌دهد. در بعضی از موارد، تفاوت میان مثال خصمانه با مثال طبیعی برای انسان قابل تشخیص نیست.

انتظار می‌رود که در آینده‌ی نزدیک، مدل‌های عمیق بسیاری در دنیای فیزیکی تعیبیه شوند. در عین حال، مطالعات و تحقیقات اخیر نشان می‌دهند که مثال‌های خصمانه را در دنیای واقعی نیز می‌توان به کار برد. برای مثال، می‌توان ورودی را به گونه‌ای تغییر داد که دستگاه‌هایی که برای گرفتن دستورات از مدل‌های شناسایی گفتار استفاده می‌کنند دستورات اشتباهی را اجرا کنند و یا باعث شد که ماشین‌های خودران در شناسایی عابران پیاده دچار اشتباه شوند. این مسئله، نگرانی‌های بسیاری را در مورد ایمنی استفاده از مدل‌های یادگیری ماشین و شبکه‌های عمیق به وجود آورده است.

در کاربردها، شبکه‌های عصبی عمیق را به صورت مدلهای جعبه سیاه در نظر می‌گیرند. به این معنی که می‌دانیم که این مدل‌ها عملکرد خوبی دارند، ولی به طرز کار درونی آن‌ها وقوف نداریم. دلیل وجود مثال‌های خصمانه یکی از پایه‌ای‌ترین مسائل  مطرح در این زمینه است. با این حال، عدم وجود درک کامل از ساز و کار این مدل‌ها و سختی‌های مرتبط با کار در فضاهای بعد بالا، توضیح و تشریح این دلایل را مشکل کرده است. دلیل وجود این پدیده هنوز یک مسئله‌ی باز است و ما خواننده را به \cite{Yuan_2019} برای مطالعه‌ی بیشتر در مورد این پدیده ارجاع می‌دهیم.

در این مقاله، ما دیدگاه جدیدی در مورد دلیل به وجود آمدن پدیده‌ی مثال‌های خصمانه در شبکه‌های عصبی را ارائه داده و مسیر جدیدی را برای مقابله با تاثیرات این پدیده ارائه می‌کنیم. این مقاله در پنج بخش تنظیم شده است. پس از مقدمه، در بخش دوم، مروری بر تعدادی از دیدگاه‌های ارائه شده‌ی مرتبط با دیدگاه پیشنهادی خواهیم داشت. سپس، در بخش سوم، دیدگاه پیشنهادی را تشریح می‌دهیم. در بخش چهارم، شواهدی  در تایید دیدگاه پیشنهادی ارائه داده و بخش پنجم را نیز به جمع‌بندی و تعیین قدم‌های بعدی در این مسیر اختصاص داده‌ایم.

\section{کارهای مرتبط}
تاکنون، برای وجود مثال‌های خصمانه سه دلیل ارائه شده است. اولین دیدگاه توسط سجدی و همکاران\cite{szegedy2013intriguing}
مطرح شد. این دیدگاه، که با نام دیدگاه پاکت‌های نادر شناخته می‌شود، به شرح زیر است. در این دیدگاه، دلیل وجودی مثال‌های خصمانه وجود پاکت‌هایی در فضای ورودی شبکه هستند که احتمال مشاهده‌ی آنها بسیار کم است و شبکه در آن نواحی خروجی مناسبی نمی‌دهد. در این دیدگاه رابطه‌ی مثال‌های خصمانه با فضای ورودی شبکه را مشابه رابطه‌ی میان اعداد گویا و اعداد حقیقی می‌دانند. به این معنی که، احتمال گویا بودن یک نمونه‌ی تصادفی از یک بازه از اعداد حقیقی صفر است، با این وجود در همسایگی هر عدد حقیقی عددی گویا یافت می‌شود. با این حال،
همان گونه که در
\cite{tanay2016boundary}
نیز به آن اشاره شده است، دلیل محکمی برای نشان دادن چنین رفتاری توسط شبکه ارائه نشده است و صرفا به مرتبط کردن آن به بسیار غیرخطی بودن نمایش یادگیری شده توسط شبکه اکتفا شده است. در 
\cite{Tabacof_2016}
 نیز شواهدی ارائه شده است که طبق این شواهد، مثال‌های خصمانه الزاما به صورت ایزوله در فضا پخش نمی‌شوند و نواحی چگال و فشرده‌ای از این مثال‌ها نیز وجود دارد. این دیدگاه چگونگی انتقال این مثال‌ها به شبکه‌های دیگر را نیز مشخص نمی‌کند.

دیدگاه دوم توسط گودفلو و همکاران\cite{goodfellow2014explaining}
مطرح شده است. این دیدگاه، که با نام دیدگاه خطی بودن شناخته می‌شود. طبق این دیدگاه، پدیده‌ی مثال‌های خصمانه در طبقه‌بندهای خطی در ابعاد بالا ظهور می‌کند و دلیل مشاهده‌ی این پدیده در شبکه‌های عصبی نیز مرتبط با علاقه‌ی ما در استفاده از الگوهای خطی در مدلسازی و آموزش این شبکه‌ها می‌باشد. گودفلو و همکاران با استفاده از این دیدگاه حمله‌ی
\lr{FGSM}
را طراحی کردند که از این خطی بودن شبکه‌های عصبی برای تولید مثال‌های خصمانه استفاده می‌کند. ضرب داخلی بین یک بردار وزن $w$ و یک مثال خصمانه‌ی $\tilde{x}=x+\eta$ را در نظر بگیرید.
\begin{equation*}
w^T\tilde{x}=w^Tx+w^T\eta
\end{equation*}
طبق معادله، اگر $\eta$ را برابر با $\mathrm{sign}(w)$ در نظر بگیریم، بیشترین تغییر را، تحت قید نورم ماکزیموم، در خروجی اعمال کرده‌ایم. خطی بودن به این معناست که شبکه رفتاری مشابه از خود نشان داده و در صورتی که $\eta$ را برابر با علامت گرادیان تابع هزینه در نقطه‌ی $x$ قرار دهیم، شبکه در طبقه‌بندی دچار خطا خواهد شد.

طبق این دیدگاه، دلیل انتقال مثال‌های خصمانه به دیگر شبکه‌ها، همگرایی این مدل‌ها به طبقه‌بند خطی بهینه است. دیدگاه خطی بودن دو پیش بینی مستقیم دارد، اول این که تمامی طبقه‌بندهای خطی از پدیده‌ی مثال‌های خصمانه رنج می‌برند و دوم این که اثر خطی بودن با افزایش ابعاد مسئله تشدید می‌شود. با این وجود، در
\cite{tanay2016boundary}
هر دوی این پیش‌بینی‌ها زیر سوال می‌روند. ابتدا مسئله‌ای  مصنوع را در نظر می‌گیرند که برای آن طبقه‌بندی خطی وجود دارد که از مثال‌های خصمانه آزاد است و در ادامه نشان می‌دهند که در یک مسئله‌ی واقعی نیز با افزایش ابعاد قدرت مثال‌های خصمانه تشدید نمی‌شود.

دیدگاه سوم توسط تانای و همکاران
\cite{tanay2016boundary}
 و با نام دیدگاه کجی مرز ارائه شده است. این دیدگاه برای طبقه‌بندهای خطی پیشنهاد شده و ظهور آن در شبکه‌های عصبی را منتج از شکلی غیرخطی از این پدیده می‌دانند. یک مسئله‌ی طبقه‌بندی دو کلاسی را در نظر بگیرید که برای آن یک طبقه‌بند خطی را آموزش داده‌ایم. این طبقه‌بند خطی یک مرز تصمیم
 $c$
 برای جدا کردن دو کلاس می‌یابد. حال طبقه‌بند بهینه‌ای را برای همان مسئله در نظر بگیرید که توسط مرکزهای دو کلاس تعریف شده باشد. این طبقه‌بند نیز یک مرز تصمیم
 $c^*$
 برای جدا کردن دو کلاس تعریف می‌کند. طبق دیدگاه کجی مرز در صورتی که مرز
 $c$
 با
 $c^*$
 زاویه داشته باشد، برای طبقه‌بند مثال‌های خصمانه وجود خواهد داشت. تانای و همکاران در ادامه نشان می‌دهند که می‌توان زاویه‌ی بین دو مرز را با منظم کردن\LTRfootnote{regularize}
طبقه‌بند کاهش داد. در پایان، تانای و همکاران با توجه به نوع و شکل اختلالات خصمانه‌ای که در شبکه‌های عصبی مشاهده می‌کنند، دلیل وجود مثال‌های خصمانه برای این شبکه‌ها را منظم نبودن این شبکه‌ها اعلام می‌کنند. با این که تانای و همکاران دلایل خوبی در تایید دیدگاه کجی مرز ارائه می‌دهند، چگونگی تعمیم آن به شبکه‌های عصبی و دیگر مدل‌های غیرخطی را مشخص نمی‌کنند. همچنین، در این دیدگاه دلیلی برای انتقال‌پذیری مثال‌های خصمانه ارائه نشده است.

وجود این ناسازگازی‌ها محققان را به سمت ارائه‌ی توجیهات دیگری سوق داده است. گروهی از محققان دلیل وجود این پدیده را با ویژگی‌های مورد استفاده در طبقه‌بندی مرتبط می‌کنند. به طور مثال، در ج؟چ شبکه را با استفاده از دودویی سازی ورودی نسبت به مثال‌های خصمانه مقاوم کردند. یا در چ؟چ دلیل وجود مثال‌های خصمانه را حضور ویژگی‌های ناپایداری می‌دانند که با برچسب خروجی همبستگی دارند. این مطالعات نشان می‌دهند که وجود مثال‌های خصمانه با داده‌های آموزشی استفاده شده و طبقه‌بند هدف ارتباط تنگاتنگی دارد. با این وجود، در حال حاضر، چارچوب یکپارچه‌ای برای توجیه و توصیف این رابطه ارائه نشده است.

همان گونه که مشاهده می‌شود، هیچ‌کدام از دیدگاه‌های مطرح شده توانایی توجیه و تشریح تمامی مشاهدات مرتبط با مثال‌های خصمانه را ندارند. با این حال، هرکدام بخش محدودی از مشاهدات را توجیه می‌کنند. در اینجا، دیدگاه جدیدی را مطرح می‌کنیم که توانایی توجیه وجود مثال‌های خصمانه و انتقال‌پذیری آن‌ها را دارد و  در عین حال، با مشاهداتی که توسط دیدگاه‌های قبلی مطرح شده نیز سازگازی دارد. در ادامه، این دیدگاه جدید را شرح می‌دهیم.

\section{تکینگی‌ها و مثال‌های خصمانه}
در این بخش، دیدگاه جدیدی را در مورد دلیل وجود مثال‌های خصمانه و انتقال‌پذیری آن‌ها ارائه می‌دهیم. در این دیدگاه وجود مثال‌های خصمانه و انتقال‌پذیری آنها نتیجه‌ی وجود تکینگی در تابع هدف به همراه وجود الگوهای فرکانس بالا در سیگنال ورودی در نظر گرفته می‌شود. مشخصه‌ی مهم مثال‌های خصمانه انتقال‌پذیری آنهاست. در اینجا، انتقال‌پذیری مثال‌های خصمانه را نتیجه‌ی غیرمستقیم اصل داربو در مورد تکینگی‌ها و بسط‌های طیفی در نظر می‌گیریم.

قبل از شرح دیدگاه پیشنهادی منظورمان از تکینگی و الگوهای فرکانس بالا را مشخص می‌کنیم. در اینجا از تعریف عام تکینگی استفاده می‌کنیم، به این معنی که ناپیوستگی‌های تابع نیز، به همراه قطب‌ها و ...، به عنوان تکینگی  در نظر گرفته می‌شوند. اگر تبدیل نمونه‌ی ورودی را در دامنه‌ی فرکانس در نظر بگیریم، الگوهای فرکانس بالا را می‌توان با اعمال یک  فیلتر بالا گذر به دست آورد.

ابتدا، با یک مثال، چگونگی به وجود آمدن مناطق خصمانه را توجیه می‌کنیم. پس از آن به شرح دقیق‌تری از مشکل مثال‌های خصمانه خواهیم پرداخت. در این راستا، یک مسئله‌ی طبقه‌بندی تعریف می‌کنیم. فرض کنید که قصد یادگیری یک طبقه‌بند برای تشخیص اعداد مثبت و منفی در بازه‌ی 
$[-1,1]$ 
را داشته باشیم. در نتیجه، تابعی که قصد یادگیری آن را داریم تابع 
$f(x)=\mathrm{sign}(x)$ 
است. همان گونه که در شکل نیز مشاهده می‌شود، تابع 
$f$ 
در نقطه‌ی مرکز مختصات یک ناپیوستگی دارد. برای آموزش شبکه به یک مجموعه‌ی داده نیاز خواهیم داشت. به این منظور، بازه‌ی 
$[-1,1]$ 
را به بازه‌های هم اندازه افراز کرده و نقاط پایانی این بازه‌ها را به عنوان مجموعه‌ی نمونه‌های آموزشی در نظر می‌گیریم.

همان گونه که در شکل مشاهده می‌شود، فرآیند آموزش در یافتن تابع مورد نظر موفق است و ناپیوستگی در مرکز مختصات نیز صاف شده است. با این وجود، مجموعه‌ی آموزشی را به گونه‌ای می‌توان تغییر داد که فرآیند آموزش در یافتن تابع مورد نظر موفق نباشد. در این راستا، ابعاد ورودی را با استفاده از توابع  پایه‌ی چبیشف افزایش می‌دهیم. در شکل نتیجه‌ی آموزش پس از افزایش ابعاد ورودی با 5 پایه‌ی اول چبیشف نشان داده شده است. همان طور که در شکل نیز مشاهده می‌شود، اضافه کردن این ابعاد باعث به وجود آمدن نوساناتی در خروجی شبکه می‌شود. در مقابل، خروجی شبکه در منطقه‌ی اطراف تکینگی به مقدار واقعی تابع نزدیک‌تر شده است. در واقع، برای جبران خطای شبکه در نزدیکی نقطه‌ی تکینگی، شبکه به الگوهای فرکانس بالا نیز حساس شده و این مسئله باعث به وجود آمدن نوسنات در خروجی شبکه می‌شود.

با دقت در نوسانات به وجود آمده، می‌توان مشاهده کرد که این نوسانات به دو شکل در خروجی خود را نشان می‌دهند. نوساناتی که در مناطق بین مثال‌های آموزشی به وجود می‌آیند و نوساناتی که در منطقه‌ی خارج از مثال‌های آموزشی به وجود می‌آیند. طبیعت این نوسانات به گونه‌ای است که با استفاده از منظم‌سازی قابل شناسایی و برطرف کردن نیستند. دلیل این مسئله این است که در نتیجه‌ی این پدیده، فرآیند منظم‌سازی نقاط بحرانی شبکه را در نزدیکی مثال‌های آموزشی قرار داده و در نتیجه اندازه‌ی مشتق در این نقاط بسیار به صفر نزدیک است. در عین حال، اندازه‌ی وزن‌های شبکه نیز کوچک است و وجود این نوسانات از این طریق نیز قابل شناسایی و تشخیص نیست.

هم اکنون، پدیده‌ی پیشنهادی را در سناریوی واقعی‌تری بررسی می‌کنیم. در این مثال، هدف ما شبیه‌سازی کاربرد برچسب‌زنی تصاویر است. در این کاربرد، دامنه‌ی ورودی ابرمکعب $[-1,1]^D$ 
است که $D$ 
در آن تعداد ابعاد ورودی است. نکته‌ای که این مسئله را از سناریوی قبلی متفاوت می‌کند این است که تمامی اطلاعات مورد نیاز برای طبقه‌بندی روی مرز دامنه‌ی ورودی قرار دارد. این موضوع نتیجه‌ی این واقعیت است که برچسب تصویر در صورت نورمال کردن تصویر با استفاده از مقدار بیشینه‌ی قدرمطلق آن تصویر تغییر نمی‌کند. علاوه بر آن، طبقه‌بندی که تصاویر را برچسب‌زنی می‌کند، عموما، تابعی زوج است. به این معنی که برچسب نمونه نگاتیو شده با نمونه طبیعی برابر است.

قرار داشتن مثال‌های طبیعی بر روی مرز ورودی در مورد تمامی کاربردهای با ابعاد بالا کم و بیش صادق است. این موضوع ناشی از این واقعیت هندسی است که اکثر جرم یک توپ چندین بعدی در نوار نازکی در نزدیکی محیط توپ قرار دارد. این واقعیت را می‌توان با محاسبه‌ی حجم کره در ابعاد بالا نشان داد. در حقیقت، در صورتی که تعداد ابعاد به بی‌نهایت نزدیک شود، حجم کره به صفر میل می‌کند.

در این مثال، مسئله‌ی طبقه‌بندی را روی دیسک واحد دو بعدی تعریف می‌کنیم. در این صورت، هر نقطه‌ی دامنه‌ی ورودی را می‌توان با مختصات قطبی $(\rho,\theta)$ 
 نشان داد که در آن $\rho \in [0,1]$ و $\theta \in [-\pi,\pi]$ است. 
 تابع هدف طبقه‌بندی را $f(\rho,\theta)=\mathrm{sign}(\sin 2\theta)$ در نظر می‌گیریم.
همان طور که گفته شد، در این مثال قصد داریم که مثال‌های آموزشی روی مرز دامنه‌ی ورودی قرار داشته باشند. به این منظور، مجموعه‌ی مثال‌های آموزشی را با افراز محیط دیسک واحد به کمان‌های مساوی تشکیل می‌دهیم. بردار ویژگی برای این مجموعه را با استفاده از مختصات کارتزی نمونه‌ها، یعنی $(x,y)=(\rho\sin\theta,\rho\cos\theta)$، تعریف می‌کنیم. نتیجه‌ی آموزش شبکه روی این مجموعه‌ی آموزشی در شکل مشاهده می‌شود.

مانند مثال قبلی، در اینجا نیز شبکه موفق به یافتن تابع هدف می‌شود. با این حال، مشابه سناریوی پیشین، ابعاد را به گونه‌ای می‌توان افزایش داد که فرآیند آموزش در یافتن طبقه‌بند هدف شکست خورده و متعاقبا مناطق خصمانه به وجود آیند. این بار اما، از چندجمله‌ای‌های پایه‌ی زرنیکه\LTRfootnote{Zernike polynomials} برای افزایش ابعاد استفاده می‌کنیم. به این منظور، ابعاد ورودی را با استفاده از پایه‌های درجه سه زرنیکه افزایش می‌دهیم که در مجموع بردار ویژگی را از 2 بعد به 6 بعد افزایش می‌دهد. از پایه‌های درجه دو استفاده نکردیم زیرا تابع هدف عضوی از پایه‌های درجه‌ی دو زرنیکه است. نتیجه‌ی آموزش شبکه با استفاده از مجموعه‌ی آموزشی افزایش بعد یافته را در شکل نشان داده‌ایم. همان گونه که در شکل نیز مشخص است، شبکه روی مرز دیسک مقدار صحیحی خروجی می‌دهد ولی اگر از مرز به سمت مرکز دیسک حرکت کنیم، برچسب ورودی قبل از عبور از مرز تصمیم واقعی تغییر می‌کند. در عمل تمامی نقاطی از ناحیه‌ی درونی دیسک که در همسایگی مرز ورودی قرار دارند به اشتباه برچسب زده شده‌اند. در مقابل، اگر از مرز به سمت بیرون دیسک حرکت کنیم، مثال خصمانه نخواهیم یافت.

این مشاهده به روش ساده‌ای برای از بین بردن نواحی خصمانه در کاربردهای مشابه اشاره می‌کند. در صورتی که بتوانیم اختلالات ناشی از غیرخطی بودن ورودی را از خروجی شبکه حذف کنیم، نواحی خصمانه از بین خواهند رفت. به همین منظور، در ادامه، ساختی از شبکه‌های عصبی را ارائه می‌دهیم که سعی در حذف کردن نوسنات ناشی از این پدیده را دارد.

\section{شبکه‌های دو مسیری}
در این بخش، ساخت جدیدی از شبکه‌های عصبی را ارائه می‌دهیم که، در کاربردهای اشاره شده در بخش قبل، تاثیر پدیده‌ی معرفی شده در خروجی را به شکل قابل توجهی کاهش می‌دهد. روش پیشنهادی از تقارن‌ها و یا ضد تقارن‌های موجود در تابع هدف برای حذف این اختلالات استفاده می‌کند. روش پیشنهادی مشابه روش‌های حذف اختلال\LTRfootnote{distortion cancellation} 
در پردازش سیگنال‌هاست. ابتدا شرح مختصری از این روش‌ها را ارائه داده و سپس روش پیشنهادی را معرفی می‌کنیم.

سیگنال‌های واقعی بسیار غیرخطی هستند و این موضوع باعث به وجود آمدن اختلال در خروجی مدارهای الکترونیکی می‌شود که این سیگنا‌ل‌ها را پردازش می‌کنند. در کاربردهای مختلف روش‌های مختلفی برای حذف این اختلالات معرفی شده است. در بسیاری از این روش‌ها از تقارن‌های موجود در سیستم استفاده می‌شود تا هارمونیک‌های ضعیف و غیر ضروری را از خروجی سیستم حذف کنند. برای نمونه، اگر در کاربرد مورد نظر هارمونیک‌های زوج قابل صرف نظر کردن باشند، تجمیع خروجی سیستم برای یک سیگنال با خروجی همان سیستم برای سیگنال آینه شده هارمونیک‌های زوج را از خروجی حذف می‌کند. حذف شدن این هارمونیک‌ها باعث صاف شدن خروجی سیستم می‌شود.

در اینجا، ما از این روش‌ها الهام گرفته و شبکه‌ی عصبی دو مسیری را معرفی می‌کنیم. همان طور که در بخش قبل اشاره کردیم، در کاربرد برچسب‌زنی تصاویر، تصاویر نگاتیو برچسب برابری با تصاویر طبیعی دارند. در اولین مرحله، تبدیلی روی فضای ورودی اعمال می‌کنیم تا مرکز مختصات را به مرکز انعکاس منتقل کنیم. پس از این انتقال، طبقه‌بند هدف یک تابع زوج خواهد بود. برای مثال، در \lr{MNIST} دامنه‌ی هر بعد ورودی پس از نورمال‌سازی بازه‌ی $[0,1]$ است. رابطه‌ی انعکاس مورد نظر به صورت $\bar{x}=1-x$ است. در نتیجه مرکز انعکاس نقطه‌ی $x=0.5$ بوده و برای انتقال آن به مرکز مختصات از تبدیل $\tilde{x}=2(x-0.5)$ استفاده می‌کنیم. هدف از تغییر مقیاس، انتقال دامنه‌ی ورودی به بازه‌ی استاندارد $[-1,1]$ است.

در مرحله‌ی بعد، با توجه به نوع تقارن موجود، از ورودی نسخه‌برداری کرده و سپس تبدیل مناسب را بر روی آن اعمال می‌کنیم. هر تابع، متعاقبا هر شبکه‌ی عصبی، را می‌توان به دو تابع زوج و فرد تجزیه کرد. برای به دست آوردن بخش‌های زوج و فرد یک تابع از روابط زیر استفاده می‌شود. در نتیجه، در کاربرد برچسب‌زنی تصاویر، تبدیل مورد نظر $T_{neg}(x)=-x$ است.
\begin{align*}
    f_e(x)&=\frac{f(x)+f(-x)}{2}\\
    f_o(x)&=\frac{f(x)-f(-x)}{2}\\
\end{align*}

تجمیع خروجی شبکه را می‌توان قبل یا بعد از لایه‌ی \lr{softmax} انجام داد. شکل تاثیر حذف هارمونیک‌های فرد یا زوج شبکه بر روی  مسئله‌ی ساختگی دو بعدی معرفی شده در بخش قبل را نشان می‌دهد. در بخش آزمایش‌ها تاثیر حذف هارمونیک‌های فرد را برای \lr{MNIST} تحلیل خواهیم کرد.

\section{طبقه‌بند‌های پایدار}
وجود پدیده‌ی مثال‌های خصمانه در شبکه‌های عصبی باعث به وجود آمدن این سوال شده است که آیا می‌توان شبکه‌ای آموزش داد که مثال‌های خصمانه نداشته باشد؟ در اینجا، طبقه‌بندی که مثال خصمانه نداشته باشد را طبقه‌بند پایدار می‌نامیم. گودفلو و همکاران\cite{goodfellow2014explaining} با تکیه بر تئوری تقریب جهانی این موضوع را مطرح می‌کنند که اگر بتوان طبقه‌بندی بدون مثال‌های خصمانه یافت، این مهم را باید بتوان توسط شبکه‌های عصبی انجام داد. هدف این بخش بررسی دقیق‌تر این موضوع است. در این راستا، ابتدا تعریفی از پایداری را برای طبقه‌بندها ارائه می‌دهیم و سپس با استفاده از این تعریف، سعی در پاسخ سوال مطرح شده در بالا خواهیم داشت.

طبقه‌بند دو کلاسی $C(\mathbf{x})$ را بر روی $\mathbb{R}^2$ در نظر بگیرید. این طبقه‌بند را می‌توان به دو تابع $u:\mathbb{R}^2 \rightarrow \mathbb{R}$ و $v:\mathbb{R}^2 \rightarrow \mathbb{R}$ تجزیه کرد. در صورتی که $u(\mathbf{x})>v(\mathbf{x})$ باشد، ورودی عضوی از کلاس اول و در صورتی که $u(\mathbf{x})<v(\mathbf{x})$ باشد، ورودی عضوی از کلاس دوم است. در صورتی که $u(\mathbf{x})=v(\mathbf{x})$ باشد، بر روی مرز تصمیم‌گیری قرار گرفته‌ایم. تابع مختلط $f:\mathbb{R}^2 \rightarrow \mathbb{C}$ را با تعریف زیر در نظر بگیرید.
\begin{equation}
    f(\mathbf{x})=u(\mathbf{x})+iv(\mathbf{x})
\end{equation}
با توجه به تعریف $u$ و $v$، رابطه‌ی $\Re f = \Im f$ مکان هندسی مرز تصمیم را تعیین می‌کند. با توجه به تعریف جهت خصمانه، در صورتی که اندازه‌ی خروجی $C(\mathbf{x}_0)$ برای یک جابه‌جایی بینهایت کوچک در تمامی جهات برابر باشد، طبقه‌بند در نقطه‌ی $\mathbf{x}_0$ جهت خصمانه نخواهد داشت. در این صورت می‌گوییم $C$ در $\mathbf{x}_0$ پایدار است. در صورتی که $C$ در تمامی نقاط دامنه‌ی خود پایدار باشد، $C$ یک طبقه‌بند پایدار خواهد بود. با کمی دقت در تعریف $f$ می‌توان مشاهده کرد که پایداری $C$ به این معناست که، در صورت یک جابه‌جایی بی‌نهایت کوچک در ورودی، اندازه‌ی تغییر خروجی $f$ برای تمام جهات به یک اندازه باشد. به عبارت دیگر، مشتق در جهت $x_1$ باید برابر با دورانی از مشتق در جهت $x_2$ باشد. در نتیجه، تابع $f$ در یکی از معادلات دیفرانسیل زیر صدق خواهد کرد.
\begin{align}
    \frac{\partial f}{\partial x_1} &= i\frac{\partial f}{\partial x_2} \label{eq:cr}\\
    \frac{\partial f}{\partial x_1} &= -i\frac{\partial f}{\partial x_2}
\end{align}
معادلات بالا معادلات بسیار شناخته شده‌ای در آنالیز مختلط هستند. معادله‌ی \ref{eq:cr} را با نام معادله‌ی کاوشی-ریمان می‌شناسیم. صدق کردن تابع $f$ در معادله‌ی کاوشی-ریمان، به همراه شرایطی در مورد پیوستگی و مشتق‌پذیری $f$، شرط لازم و کافی برای هولومورفیک بودن تابع $f$ است. توابع هولومورفیک

\section{شواهد و آزمایش‌ها}
در این بخش، شواهدی در تایید رابطه‌ی پدیده‌ی مثال‌های خصمانه و دیدگاه پیشنهادی را ارائه می‌دهیم. همان گونه که در بخش قبل به آن اشاره شد، طبق دیدگاه پیشنهادی، دو روش عمده‌ی مقابله با پدیده‌ی مثال‌های خصمانه منظم‌سازی و روش کمترین مربعات است. با این  حال، وجود این رابطه را بدون استفاده از روش‌های اشاره شده نیز می‌توان نشان داد. با تکیه بر دیدگاه پیشنهادی، هنگام طبقه‌بندی تصاویر با استفاده از شبکه‌های عصبی، می‌توان قید بسیار قویی به شبکه اضافه کرد که شبکه را نسبت به تمامی حملات مبنی بر شهود گرادیان  شبکه، از جمله حمله‌ی \lr{FGSM}، ایمن کند.

می‌دانیم که، در کاربرد طبقه‌بندی تصاویر، دامنه‌ی ورودی شبکه ابرمکعب $[-1,1]^D$ است و $D$ بعد ورودی را نشان می‌دهد. در این کاربرد، می‌توان فرض کرد که برچسب هر نقطه با برچسب نزدیکترین نقطه روی سطح ابرمکعب برابر است. نزدیک‌ترین نقطه روی سطح ابرمکعب به مثال طبیعی را می‌توان با نورمال کردن مقادیر پیکسل‌ها، به گونه‌ای که بزرگترین مقدار مطلق پیکسل‌ها برابر با یک شود به دست آورد. در داده‌های آموزشی ساده‌تر، مانند 
\lr{MNIST}، 
می‌توان فرض کرد که برچسب هر نقطه‌ی روی سطح ابرمکعب با برچسب نزدیکترین گوشه برابر است. در نتیجه، تعداد مثال‌های طبیعی حداکثر به تعداد گوشه‌های این ابرمکعب، یعنی $2^D$ است. طبق دیدگاه پیشنهادی، پدیده‌ی مثال‌های خصمانه ناشی از همگرایی نقطه‌ای شبکه‌ها‌ی یافت شده به میانگین شبکه‌های درون‌یاب گذرنده از این گوشه‌ها است. در این صورت، شبکه‌ای که بر روی مثال‌های طبیعی آموزش داده شده است،  طبقه‌بند خوبی برای نزدیک‌ترین گوشه‌ها به مثال‌های طبیعی خواهد بود. 

 این موضوع را به راحتی می‌توان آزمایش کرد. آزمایش نشان می‌دهد که عملکرد شبکه‌ها بر روی نزدیک‌ترین گوشه‌ها به مثال‌های آموزشی، با حاشیه‌ی بسیار کوچکی، تقریبا برایر با عملکرد شبکه بر روی مثال‌های طبیعی آموزشی است. فقط برچسب تعداد کمی از مثال‌های طبیعی پس از این جابه‌جایی تغییر می‌کند و تصاویر گوشه‌ای که برچسب آنها تغییر می‌کند مثال‌های خصمانه‌ای برای این مثال‌های طبیعی هستند. این مثال‌های خصمانه را می‌توان ناشی از کجی مرز دانست زیرا که مقدار خروجی شبکه برای این مثال‌های طبیعی خیلی بزرگ نیست و اکثر این مثال‌ها در همسایگی مرز تصمیم قرار دارند. در بقیه‌ی موارد اما، مثال‌های خصمانه را نمی‌توان ناشی از کجی مرز دانست، زیرا طبقه‌بند نقاطی خارج از ناحیه‌ی محدب تعریف شده توسط مثال‌های آموزشی را به درستی برچسب می‌زند که نشان از تعمیم‌پذیری مرز یاد گرفته شده دارد.

اگر بخواهیم خروجی شبکه همیشه با مقدار آن در نزدیکترین گوشه برابر باشد، کافیست که تمامی مقادیر مشتقات شبکه در گوشه‌ها را برابر با صفر کنیم. در این راستا، توابع پایه‌ی لایه‌ی ورودی شبکه را به گونه‌ای تغییر می‌دهیم که تمامی شبکه‌های ممکن در قید فوق صدق کنند. با تغییر پایه‌ی خطی $x$ به پایه‌ی $\mathrm{sign}(x)$ تمامی مشتقات شبکه نسبت به ورودی، به غیر از نقاط روی صفحات مختصات، در تمامی ناحیه‌ی دامنه‌ی ورودی برابر با صفر خواهد بود. مشتق شبکه نسبت به $x$ در نقاط روی صفحات مختصات به دلیل ناپیوستگی تابع $\mathrm{sign}(x)$ وجود ندارد. این شبکه‌ها را شبکه‌های کوانتیزه شده می‌نامیم. در صورتی که بخواهیم شبکه مشتق‌پذیر باشد، می‌توانیم از تبدیل $\sin(\frac{\pi x}{2})$ استفاده کنیم. در این صورت مشتق شبکه نسبت به $x$ برابر با $\frac{\pi}{2}N_n'(\sin(\frac{\pi x}{2}))\cos(\frac{\pi x}{2})$ خواهد بود که مقدار آن در گوشه‌ها صفر است ولی مشتقات دوم و بالاتر در گوشه‌ها مقدار غیر صفر دارد. شبکه‌هایی که فقط چند مشتق آنها در گوشه‌ها برابر صفر است را شبکه‌های شبه کوانتیزه شده می‌نامیم.

همان طور که مشاهده شد، می‌توان شبکه‌هایی ساخت که مشتق آنها نسبت به ورودی، در صورت وجود، برابر با صفر باشد. با این حال، مشتق این شبکه‌ها همچنان نسبت به وزن‌ها مقدار معنی‌داری دارد. در نتیجه، تغییر اعمال شده باعث پیچیده‌تر شدن آموزش نمی‌شود. فارق از این که بر روی چه زیر مجموعه‌ای از گوشه‌ها آموزش را انجام می‌دهیم، گوشه‌هایی که عضو این مجموعه نباشند خارج از ناحیه‌ی محدب تعریف شده توسط مجموعه‌ی آموزشی قرار می‌گیرند. این موضوع نشان می‌دهد که مسئله‌ی طبقه‌بندی تصاویر بیشتر از آن که یک مسئله‌ی درون‌یابی باشد، یک مسئله‌ی برون‌یابی است.

با کمی دقت در ساخت شبکه‌های کوانتیزه شده، می‌توان مشاهده کرد که تمامی اطلاعاتی که شبکه یاد گرفته است روی مرز دامنه‌ی ورودی قرار دارد. در نتیجه، طبقه‌بند تعریف شده توسط شبکه‌ی عصبی کوانتیزه شده را می‌توان معادل خمی بر روی مرز دامنه‌ی ورودی در نظر گرفت که مقدار آن به صورت آنالتیک به ناحیه‌ی درونی دامنه تعمیم داده شده است. در واقع با کوانتیزه کردن ورودی، تمام فضای درون ابرمکعب $D$ بعدی، به همراه تمامی نوسانات حاصل از درون‌یابی در این ناحیه، درون یک تکینگی در مرکز مختصات فشرده شده است. در نتیجه، تمامی مثال‌های خصمانه‌ی درون ابرمکعب از بین خواهند رفت. با این وجود خم تعریف شده توسط شبکه روی مرزها همچنان می‌تواند تغییر کند. این خم که شکلی پله‌ای دارد می‌تواند نسبت به خطای مطرح شده در دیدگاه پیشنهادی یا دیدگاه کجی مرز حساس باشد.

شبکه‌های کوانتیزه شده طبق تعریف نسبت به حمله‌ی \lr{FGSM} ایمن هستند. حل دقیق مسئله‌ی یافتن مثال خصمانه برای شبکه‌های عصبی عادی یک مسئله‌ی سخت است\cite{szegedy2013intriguing}. با این حال، حل تقریبی آن سخت نیست. این موضوع در مورد شبکه‌های کوانتیزه شده صدق نمی‌کند. در این شبکه‌ها، حل تقریبی مسئله‌ی یافتن مثال‌های خصمانه نیز سخت است و باید از روش‌های تصادفی، مانند الگوریتم‌های ژنتیک، برای حل مسئله‌ی بهینه‌سازی استفاده کرد. دلیل این موضوع نبود اطلاعات گرادیان و ثابت محلی بودن  شبکه‌ی کوانتیزه شده است. البته این موضوع به این معنی نیست که در عمل نیز یافتن چنین گوشه‌ای سخت باشد و این مسئله با خوب بودن طبقه‌بند رابطه‌ی مستقیم دارد.

در ادامه تاثیر کوانتیزه کردن ورودی بر عملکرد حمله‌ی \lr{FGSM} را بررسی می‌کنیم. همان گونه که اشاره شد، شبکه‌ی کاملا کوانتیزه شده نسبت به این حمله ایمن است. در نتیجه، تقریبی از این شبکه‌ها را برای بررسی تاثیرات آن استفاده می‌کنیم. برای این آزمایش سه مدل را برای آموزش بر روی
\lr{MNIST}
در نظر می‌گیریم. هر کدام از مدل‌ها را با استفاده از پایه‌های تک‌جمله‌ای $x$ و مثلثاتی $\sin(\frac{\pi x}{2})$ آموزش می‌دهیم. استفاده از توابع پایه‌ی تک‌جمله‌ای معادل استفاده از لایه‌ی ورودی عادی و منظور از استفاده از توابع مثلثاتی اعمال تبدیل $\sin(\frac{\pi x}{2})$ در لایه‌ی ورودی می‌باشد. سپس به هر مدل با استفاده از روش
\lr{FGSM}
حمله کرده و  دقت مدل قبل و بعد از حمله را ثبت کردیم. برای انجام حمله از پیاده‌سازی موجود در کتابخانه‌ی معرفی شده در 
\cite{papernot2018cleverhans}
استفاده شد. این آزمایش را برای هر مدل ده بار تکرار کردیم. نتایج در جدول
\ref{tbl:sizes}
گزارش شده است. همان گونه که مشاهده می‌شود، نتایج آزمایش پیش‌بینی دیدگاه پیشنهادی را تایید می‌کند.

\begin{table}[]
    \begin{latin}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{|l|l|l|l|l|l|}
        \hline
        \multirow{2}{*}{Model} & \multirow{2}{*}{Size}              & \multicolumn{2}{l|}{Smooth}     & \multicolumn{2}{l|}{Sudo-Quantized}           \\ \cline{3-6} 
                               &                                    & Clean Acc.     & Adversarial Acc. & Clean Acc.     & Adversarial Acc.        \\ \hline
        Perceptron               & $784\times10$                      & $92.39\pm0.07$ & $3.99\pm0.04$    & $91.83\pm0.18$ & $\mathbf{33.84\pm1.69}$ \\ \hline
        FC                     & $784\times 200\times 200\times 10$ & $97.76\pm0.08$ & $1.16\pm0.100$   & $97.25\pm0.24$ & $\mathbf{51.05\pm1.85}$ \\ \hline
        CNN                    & $784\times 200\times 200\times 10$ & $99.21\pm0.20$ & $4.14\pm1.61$    & $99.03\pm0.12$ & $\mathbf{66.07\pm3.11}$ \\ \hline
    \end{tabular}
    }
    \end{latin}
    \caption{مدل‌های استفاده شده در آزمایش، ابعاد آنها و دقت آنها قبل و پس از حمله‌ی \lr{FGSM}. همان طور که مشاهده می‌شود، کوانتیزه کردن ورودی تاثیر بسیار قابل توجهی بر روی شانس موفقیت حمله دارد.}
    \label{tbl:sizes}
\end{table}

در دومین آزمایش، یک شبکه‌ی پرسپترون چند لایه را در نظر می‌گیریم. هدف مقایسه‌ی ماهیت و شکل حمله در صورت کوانتیزه کردن ورودی است. در این آزمایش نیز از شبکه‌ی شبه کوانتیزه شده‌ی آزمایش قبل به جای شبکه‌ی کوانتیزه شده استفاده می‌کنیم. نتایج این آزمایش در شکل \ref{fig:shape} نشان داده شده‌اند. همان گونه که در شکل نیز مشخص است، طبقه‌بند عادی به مقادیر الگوی ورودی در تمامی ابعاد حساس است. در مقابل، طبقه‌بند شبه کوانتیزه شده فقط به ابعادی در الگو که مقدار آنها به مکان ناپیوستگی ناشی از کوانتیزه کردن فضای ورودی نزدیک باشد، یعنی لبه‌های تصویر، حساس است.

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{shapeshift.png}
	\caption{شکل و ماهیت حملات قبل و پس از تغییر توابع پایه‌ی ورودی. ردیف بالا مثال‌های طبیعی، ردیف وسط شکل حمله در صورت استفاده از توابع پایه‌ی تک‌جمله‌ای و ردیف آخر ماهیت حمله در صورت استفاده از توابع پایه‌ی مثلثاتی را نشان می‌دهند.}
	\label{fig:shape}
\end{figure}

نکته‌ی دیگری که در شکل دیده می‌شود، مشابه بودن رفتار حمله در انتخاب جهت اختلال خصمانه است. با کمی دقت، به نظر می‌رسد پیکسل‌هایی که در دو حالت مشترک هستند، رفتار مشابهی نیز از خود نشان می‌دهند و در هر دو حالت مکان پیکسل در تصویر  شدت و علامت آن را تعیین می‌کند. این موضوع با نتیجه‌ای که در \cite{goodfellow2014explaining} در مورد اهمیت جهت ورودی به دست آمده سازگاری دارد.

\section{جمع‌بندی}
در این مقاله دیدگاه جدیدی را در مورد چگونگی به وجود آمدن مثال‌های خصمانه و انتقال‌پذیری آنها در شبکه‌های عصبی را ارائه دادیم. طبق دیدگاه جدید، وجود مثال‌های خصمانه اثر جانبی همگرایی نقطه‌ای شبکه‌های عصبی بر روی مثال‌های طبیعی و انتقال‌پذیری آنها نتیجه‌ی همگرایی فرآیند آموزش به میانگینی از توابع درون‌یاب مثال‌های طبیعی و منظم بودن فاصله‌ی بین این مثال‌ها در نظر گرفته می‌شود. طبق نظریه‌ی تقریب جهانی، شبکه‌های عصبی توانایی تقریب زدن هر تابعی را دارند. با این حال، شواهد نشان می‌دهد در صورتی که مثال‌های طبیعی به صورت منظم در فضای ورودی توزیع شده باشند، یافتن طبقه‌بندی مناسب از میان تمامی طبقه‌بندهای نامناسب با چالش‌هایی، مستقل از مدل یادگیری، رو به رو است.

با در نظر گرفتن شباهات دیدگاه پیشنهادی و پدیده‌ی رونگه، روش‌های مقابله با مثال‌های خصمانه به دو گروه روش‌های کمترین مربعات و منظم‌سازی تقسیم می‌شوند. با این حال، در اجرای هر دوی این استراتژی‌ها مشکلاتی وجود دارد. در روش کمترین مربعات، نیاز به تولید مثال‌هایی خیالی داریم. از این لحاظ، آموزش خصمانه را می‌توان نوعی از روش‌های کمترین مربعات دانست. در مقابل، برای اجرای استراتژی منظم‌سازی نیاز به روش آموزشی خواهیم داشت که خروجی شبکه را در نقاطی ثابت نگه داشته و باقی درجات آزادی را برای ارضای هدف منظم‌سازی هزینه کند.

در ادامه، با تکیه بر دیدگاه پیشنهادی، کوانتیزه کردن ورودی را به عنوان یک استراتژی ارزان قیمت و موثر در کاهش اثرات مخرب پدیده‌ی مثال‌های خصمانه مورد بررسی قرار دادیم. شبکه‌های کوانتیزه شده در برابر حمله‌های بر پایه‌ی گرادیان ایمن هستند. در این شبکه‌ها، با احتمال قریب به یقین بردار گرادیان شبکه نسبت به ورودی برابر با صفر است. در نتیجه، مسئله‌ی یافتن مثال خصمانه برای این شبکه‌ها به یک مسئله‌ی بهینه‌سازی اعداد صحیح تبدیل می‌شود که حل آن نسبت به مسئله‌ی مشابه که بر روی اعداد حقیقی تعریف شده باشد سخت‌تر است. با این وجود، کوانتیزه کردن ورودی فقط مثال‌های خصمانه‌ی ناشی از درون‌یابی را از بین می‌برد و مثال‌های خصمانه‌ای که ناشی از ضعف برون‌یابی باشند را می‌توان همچنان با استفاده از روش‌های بهینه‌سازی اعداد صحیح و یا با استفاده از انتقال‌پذیری مثال‌های خصمانه به دست آورد. کوانتیزه کردن ورودی را می‌توان حالت خاصی از روش‌های مخفی کردن گرادیان برای ایمن‌سازی مدل‌های یادگیری ماشین در نظر گرفت.

دیدگاه پیشنهادی، برای کاربرد طبقه‌بندی تصاویر، روش مشخصی را برای مقاوم کردن شبکه نسبت به حملات خصمانه پیشنهاد می‌دهد. در این کاربرد، در صورتی که برچسب نقاط روی مرز دامنه‌ی ورودی را داشته باشیم، می‌توان برچسب نقاط فضای درونی ورودی را با تعمیم آنالتیک برچسب مرزها تعیین کرد. در نتیجه، در صورتی که ساختار شبکه یا روش آموزش را به گونه‌ای طراحی کنیم که شبکه‌ی یافت شده قید فوق را ارضا کند، بخش قابل توجهی از مثال‌های خصمانه از بین خواهند رفت. با این وجود، شواهد حاکی از آن است که مثال‌های خصمانه ممکن است به دلایل متفاوتی به وجود آیند. این موضوع نشان می‌دهد که روش‌های دیگری نیز برای بروز مثال‌های خصمانه، علاوه‌بر دیدگاه‌های مطرح شده، وجود دارند. خوشبختانه، می‌توان امیدوار بود که منظم‌سازی و افزایش مجموعه‌ی آموزشی تکنیک‌های کلی مبارزه با مثال‌های خصمانه باشند.

\setLTRbibitems
\bibliographystyle{plain}
\bibliography{refs.bib}

\end{document}